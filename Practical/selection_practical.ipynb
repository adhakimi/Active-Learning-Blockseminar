{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Understand active learning and informative sample selection\n",
    "- Use LLMs to identify valuable training examples\n",
    "- Compare different selection strategies\n",
    "- Evaluate the impact of selected samples on model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LOAD LLM MODEL (Qwen-2.5-0.5B Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_model(model_name=\"Qwen/Qwen2.5-0.5B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Load Qwen-2.5-0.5B Instruct model for query selection\n",
    "    This model is small enough to run on Google Colab\n",
    "    \"\"\"\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "        print(f\"Model loaded successfully!\")\n",
    "        return tokenizer, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Falling back to a smaller alternative...\")\n",
    "        # Fallback to a smaller model if needed\n",
    "        return None, None\n",
    "\n",
    "# Load the LLM\n",
    "llm_tokenizer, llm_model = load_llm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SAMPLE DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from textstat import flesch_reading_ease\n",
    "import re\n",
    "\n",
    "def create_diverse_imdb_dataset(n_samples=10000, test_split=False):\n",
    "    \"\"\"\n",
    "    Create a diverse text classification dataset using the actual IMDB dataset\n",
    "    with varying difficulty levels based on review characteristics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load IMDB dataset\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    split = \"test\" if test_split else \"train\"\n",
    "    imdb_data = dataset[split]\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    difficulty = []\n",
    "    \n",
    "    # Process samples from IMDB dataset\n",
    "    indices = random.sample(range(len(imdb_data)), min(n_samples * 2, len(imdb_data)))\n",
    "    \n",
    "    for idx in indices:\n",
    "        if len(texts) >= n_samples:\n",
    "            break\n",
    "            \n",
    "        text = imdb_data[idx]['text']\n",
    "        label = imdb_data[idx]['label']  # 0 = negative, 1 = positive\n",
    "        \n",
    "        # Clean text (remove HTML tags, extra whitespace)\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Skip very short reviews\n",
    "        if len(text.split()) < 10:\n",
    "            continue\n",
    "            \n",
    "        # Determine difficulty based on review characteristics\n",
    "        difficulty_level = classify_difficulty(text, label)\n",
    "        \n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "        difficulty.append(difficulty_level)\n",
    "    \n",
    "    # Ensure we have the requested number of samples\n",
    "    if len(texts) < n_samples:\n",
    "        # If we need more samples, cycle through what we have\n",
    "        while len(texts) < n_samples:\n",
    "            idx = len(texts) % len(imdb_data)\n",
    "            text = re.sub(r'<[^>]+>', '', imdb_data[idx]['text'])\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "            if len(text.split()) >= 10:\n",
    "                texts.append(text)\n",
    "                labels.append(imdb_data[idx]['label'])\n",
    "                difficulty.append(classify_difficulty(text, imdb_data[idx]['label']))\n",
    "    \n",
    "    # Trim to exact size\n",
    "    texts = texts[:n_samples]\n",
    "    labels = labels[:n_samples]\n",
    "    difficulty = difficulty[:n_samples]\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    combined = list(zip(texts, labels, difficulty))\n",
    "    random.shuffle(combined)\n",
    "    texts, labels, difficulty = zip(*combined)\n",
    "    \n",
    "    return list(texts), list(labels), list(difficulty)\n",
    "\n",
    "def classify_difficulty(text, label):\n",
    "    \"\"\"\n",
    "    Classify the difficulty of a review based on various characteristics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Word indicators for clear sentiment\n",
    "    strong_positive = ['amazing', 'fantastic', 'brilliant', 'outstanding', 'perfect', \n",
    "                      'excellent', 'wonderful', 'incredible', 'magnificent', 'superb']\n",
    "    strong_negative = ['terrible', 'awful', 'horrible', 'worst', 'hate', 'disgusting',\n",
    "                      'pathetic', 'garbage', 'trash', 'atrocious']\n",
    "    \n",
    "    # Ambiguity indicators\n",
    "    mixed_indicators = ['but', 'however', 'although', 'despite', 'nevertheless', \n",
    "                       'on the other hand', 'mixed', 'somewhat', 'partially']\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Count strong sentiment words\n",
    "    strong_pos_count = sum(1 for word in strong_positive if word in text_lower)\n",
    "    strong_neg_count = sum(1 for word in strong_negative if word in text_lower)\n",
    "    mixed_count = sum(1 for phrase in mixed_indicators if phrase in text_lower)\n",
    "    \n",
    "    # Calculate reading complexity\n",
    "    try:\n",
    "        reading_ease = flesch_reading_ease(text)\n",
    "    except:\n",
    "        reading_ease = 50  # Default middle value\n",
    "    \n",
    "    # Length factor\n",
    "    word_count = len(text.split())\n",
    "    \n",
    "    # Determine difficulty\n",
    "    if label == 1:  # Positive review\n",
    "        if strong_pos_count >= 2 and mixed_count == 0 and reading_ease > 60:\n",
    "            return 'easy'\n",
    "        elif strong_neg_count > 0 or mixed_count >= 2 or reading_ease < 30:\n",
    "            return 'hard'\n",
    "        else:\n",
    "            return 'medium'\n",
    "    else:  # Negative review\n",
    "        if strong_neg_count >= 2 and mixed_count == 0 and reading_ease > 60:\n",
    "            return 'easy'\n",
    "        elif strong_pos_count > 0 or mixed_count >= 2 or reading_ease < 30:\n",
    "            return 'hard'\n",
    "        else:\n",
    "            return 'medium'\n",
    "\n",
    "def get_difficulty_distribution(difficulty_list):\n",
    "    \"\"\"\n",
    "    Get the distribution of difficulty levels in the dataset\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    return Counter(difficulty_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels, difficulty = create_diverse_imdb_dataset(n_samples=100000)\n",
    "    \n",
    "print(f\"Dataset created with {len(texts)} samples\")\n",
    "print(f\"Label distribution: {sum(labels)} positive, {len(labels) - sum(labels)} negative\")\n",
    "print(f\"Difficulty distribution: {get_difficulty_distribution(difficulty)}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample reviews:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nReview {i+1} ({'Positive' if labels[i] else 'Negative'}, {difficulty[i]}):\")\n",
    "    print(texts[i][:200] + \"...\" if len(texts[i]) > 200 else texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LLM-BASED QUERY SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMQuerySelector:\n",
    "    def __init__(self, tokenizer, model, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "    def generate_response(self, prompt, max_length=512, temperature=0.7):\n",
    "        \"\"\"Generate response from LLM\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=max_length,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            # Remove the input prompt from response\n",
    "            response = response[len(prompt):].strip()\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def select_informative_samples(self, candidate_texts, n_select=10, strategy=\"informativeness\"):\n",
    "        \"\"\"\n",
    "        Use LLM to select most informative samples for training\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create different prompts based on strategy\n",
    "        if strategy == \"informativeness\":\n",
    "            system_prompt = \"\"\"\"\"\"\n",
    "\n",
    "        elif strategy == \"diversity\":\n",
    "            system_prompt = \"\"\"\"\"\"\n",
    "            \n",
    "        elif strategy == \"difficulty\":\n",
    "            system_prompt = \"\"\"\"\"\"\n",
    "        \n",
    "        # Format candidate texts\n",
    "        text_list = \"\\\\n\".join([f\"{i}: {text[:100]}...\" if len(text) > 100 else f\"{i}: {text}\" \n",
    "                               for i, text in enumerate(candidate_texts)])\n",
    "        \n",
    "        full_prompt = f\"\"\"{system_prompt}\n",
    "\n",
    "        Here are the candidate movie reviews:\n",
    "        {text_list}\n",
    "\n",
    "        Please select {n_select} indices of the most suitable samples:\"\"\"\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.generate_response(full_prompt, max_length=200, temperature=0.3)\n",
    "        \n",
    "        # Parse indices from response\n",
    "        try:\n",
    "            # Extract numbers from response\n",
    "            indices = re.findall(r'\\\\b\\\\d+\\\\b', response)\n",
    "            indices = [int(idx) for idx in indices if int(idx) < len(candidate_texts)]\n",
    "            \n",
    "            # Ensure we have the right number of indices\n",
    "            if len(indices) < n_select:\n",
    "                # Fill with random indices if not enough found\n",
    "                available_indices = list(range(len(candidate_texts)))\n",
    "                missing_count = n_select - len(indices)\n",
    "                additional_indices = random.sample([i for i in available_indices if i not in indices], \n",
    "                                                 min(missing_count, len(available_indices) - len(indices)))\n",
    "                indices.extend(additional_indices)\n",
    "            \n",
    "            return indices[:n_select]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing LLM response: {e}\")\n",
    "            print(f\"Response was: {response}\")\n",
    "            # Fallback to random selection\n",
    "            return random.sample(range(len(candidate_texts)), n_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TRADITIONAL UNCERTAINTY SAMPLING (BASELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraditionalQuerySelector:\n",
    "    def __init__(self):\n",
    "        self.model = LogisticRegression(random_state=42)\n",
    "        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "        \n",
    "    def fit(self, texts, labels):\n",
    "        \"\"\"Fit the baseline model\"\"\"\n",
    "        X = self.vectorizer.fit_transform(texts)\n",
    "        self.model.fit(X, labels)\n",
    "        \n",
    "    def uncertainty_sampling(self, candidate_texts, n_select=10):\n",
    "        \"\"\"Traditional uncertainty sampling using logistic regression\"\"\"\n",
    "        X = self.vectorizer.transform(candidate_texts)\n",
    "        probabilities = self.model.predict_proba(X)\n",
    "        \n",
    "        # Calculate entropy (uncertainty)\n",
    "        entropy = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n",
    "        \n",
    "        # Select most uncertain samples\n",
    "        uncertain_indices = np.argsort(entropy)[-n_select:]\n",
    "        return uncertain_indices.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ACTIVE LEARNING COMPARISON EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_active_learning_comparison(texts, labels, difficulty, \n",
    "                                 initial_size=50, query_size=20, max_iterations=10):\n",
    "    \"\"\"\n",
    "    Compare LLM-based selection with traditional uncertainty sampling\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize selectors\n",
    "    if llm_model is not None:\n",
    "        llm_selector = LLMQuerySelector(llm_tokenizer, llm_model, device)\n",
    "    else:\n",
    "        print(\"LLM model not available, skipping LLM-based selection\")\n",
    "        llm_selector = None\n",
    "        \n",
    "    traditional_selector = TraditionalQuerySelector()\n",
    "    \n",
    "    # Split data\n",
    "    total_indices = list(range(len(texts)))\n",
    "    random.shuffle(total_indices)\n",
    "    \n",
    "    # Test set (20%)\n",
    "    test_size = len(texts) // 5\n",
    "    test_indices = total_indices[:test_size]\n",
    "    train_pool_indices = total_indices[test_size:]\n",
    "    \n",
    "    # Initial labeled pool\n",
    "    initial_indices = train_pool_indices[:initial_size]\n",
    "    unlabeled_indices = train_pool_indices[initial_size:]\n",
    "    \n",
    "    test_texts = [texts[i] for i in test_indices]\n",
    "    test_labels = [labels[i] for i in test_indices]\n",
    "    \n",
    "    # Results storage\n",
    "    results = {\n",
    "        'method': [],\n",
    "        'iteration': [],\n",
    "        'labeled_size': [],\n",
    "        'accuracy': [],\n",
    "        'f1_score': [],\n",
    "        'selected_difficulty': []\n",
    "    }\n",
    "    \n",
    "    print(\"Starting Active Learning Comparison...\")\n",
    "    \n",
    "    # LLM-based strategies to test\n",
    "    llm_strategies = ['informativeness', 'diversity', 'difficulty'] if llm_selector else []\n",
    "    \n",
    "    for strategy in ['traditional'] + llm_strategies:\n",
    "        print(f\"\\\\n--- Testing {strategy} strategy ---\")\n",
    "        \n",
    "        # Reset pools for each strategy\n",
    "        current_labeled = initial_indices.copy()\n",
    "        current_unlabeled = unlabeled_indices.copy()\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            print(f\"Iteration {iteration + 1}: {len(current_labeled)} labeled samples\")\n",
    "            \n",
    "            # Get current labeled data\n",
    "            labeled_texts = [texts[i] for i in current_labeled]\n",
    "            labeled_labels = [labels[i] for i in current_labeled]\n",
    "            \n",
    "            # Train classifier\n",
    "            vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "            X_train = vectorizer.fit_transform(labeled_texts)\n",
    "            X_test = vectorizer.transform(test_texts)\n",
    "            \n",
    "            classifier = LogisticRegression(random_state=42)\n",
    "            classifier.fit(X_train, labeled_labels)\n",
    "            \n",
    "            # Evaluate\n",
    "            predictions = classifier.predict(X_test)\n",
    "            accuracy = accuracy_score(test_labels, predictions)\n",
    "            f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "            \n",
    "            # Analyze selected samples difficulty - FIXED\n",
    "            if iteration > 0:\n",
    "                # Get the last query_size samples that were added\n",
    "                last_selected = current_labeled[-query_size:]\n",
    "                selected_difficulties = [difficulty[i] for i in last_selected]\n",
    "            else:\n",
    "                # For the first iteration, analyze the initial samples\n",
    "                selected_difficulties = [difficulty[i] for i in current_labeled]\n",
    "            \n",
    "            difficulty_dist = pd.Series(selected_difficulties).value_counts().to_dict()\n",
    "            \n",
    "            # Store results\n",
    "            results['method'].append(strategy)\n",
    "            results['iteration'].append(iteration + 1)\n",
    "            results['labeled_size'].append(len(current_labeled))\n",
    "            results['accuracy'].append(accuracy)\n",
    "            results['f1_score'].append(f1)\n",
    "            results['selected_difficulty'].append(str(difficulty_dist))\n",
    "            \n",
    "            print(f\"  Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "            print(f\"  Selected difficulty distribution: {difficulty_dist}\")\n",
    "            \n",
    "            # Query new samples (if not last iteration)\n",
    "            if iteration < max_iterations - 1 and len(current_unlabeled) >= query_size:\n",
    "                candidate_texts = [texts[i] for i in current_unlabeled]\n",
    "                \n",
    "                if strategy == 'traditional':\n",
    "                    # Traditional uncertainty sampling\n",
    "                    traditional_selector.fit(labeled_texts, labeled_labels)\n",
    "                    selected_local_indices = traditional_selector.uncertainty_sampling(\n",
    "                        candidate_texts, n_select=query_size\n",
    "                    )\n",
    "                    \n",
    "                elif strategy in llm_strategies and llm_selector:\n",
    "                    # LLM-based selection\n",
    "                    selected_local_indices = llm_selector.select_informative_samples(\n",
    "                        candidate_texts, n_select=query_size, strategy=strategy\n",
    "                    )\n",
    "                \n",
    "                # Convert local indices to global indices\n",
    "                selected_global_indices = [current_unlabeled[i] for i in selected_local_indices]\n",
    "                \n",
    "                # Update pools\n",
    "                current_labeled.extend(selected_global_indices)\n",
    "                current_unlabeled = [idx for idx in current_unlabeled if idx not in selected_global_indices]\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Usage example - make sure to use the correct variable name\n",
    "print(\"Starting comprehensive comparison...\")\n",
    "comparison_results = run_active_learning_comparison(\n",
    "    texts, labels, difficulty,  # Changed from difficulty_levels to difficulty\n",
    "    initial_size=50, query_size=20, max_iterations=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. RESULTS ANALYSIS AND VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\nExperiment Results:\")\n",
    "print(comparison_results.head(10))\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "for method in comparison_results['method'].unique():\n",
    "    method_data = comparison_results[comparison_results['method'] == method]\n",
    "    plt.plot(method_data['labeled_size'], method_data['accuracy'], \n",
    "             marker='o', linewidth=2, label=method.title())\n",
    "\n",
    "plt.xlabel('Number of Labeled Samples')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Active Learning Strategies: Accuracy Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "for method in comparison_results['method'].unique():\n",
    "    method_data = comparison_results[comparison_results['method'] == method]\n",
    "    plt.plot(method_data['labeled_size'], method_data['f1_score'], \n",
    "             marker='s', linewidth=2, label=method.title())\n",
    "\n",
    "plt.xlabel('Number of Labeled Samples')\n",
    "plt.ylabel('Test F1 Score')\n",
    "plt.title('Active Learning Strategies: F1 Score Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Sample efficiency analysis\n",
    "plt.subplot(2, 2, 3)\n",
    "final_results = comparison_results.groupby('method').last()\n",
    "methods = final_results.index\n",
    "accuracies = final_results['accuracy']\n",
    "sample_counts = final_results['labeled_size']\n",
    "\n",
    "bars = plt.bar(methods, accuracies, alpha=0.7)\n",
    "plt.ylabel('Final Test Accuracy')\n",
    "plt.title('Final Performance Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add sample count annotations\n",
    "for bar, count in zip(bars, sample_counts):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'n={count}', ha='center', va='bottom')\n",
    "\n",
    "# Method comparison table\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.axis('off')\n",
    "summary_data = []\n",
    "for method in comparison_results['method'].unique():\n",
    "    method_data = comparison_results[comparison_results['method'] == method]\n",
    "    final_acc = method_data['accuracy'].iloc[-1]\n",
    "    final_f1 = method_data['f1_score'].iloc[-1]\n",
    "    final_samples = method_data['labeled_size'].iloc[-1]\n",
    "    summary_data.append([method.title(), f\"{final_acc:.4f}\", f\"{final_f1:.4f}\", final_samples])\n",
    "\n",
    "table = plt.table(cellText=summary_data,\n",
    "                 colLabels=['Method', 'Final Accuracy', 'Final F1', 'Samples Used'],\n",
    "                 cellLoc='center',\n",
    "                 loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "plt.title('Summary Statistics')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. DETAILED ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"DETAILED ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample efficiency analysis\n",
    "print(\"\\\\nSample Efficiency Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "for method in comparison_results['method'].unique():\n",
    "    method_data = comparison_results[comparison_results['method'] == method]\n",
    "    final_accuracy = method_data['accuracy'].iloc[-1]\n",
    "    samples_used = method_data['labeled_size'].iloc[-1]\n",
    "    print(f\"{method.title():15s}: {final_accuracy:.4f} accuracy with {samples_used} samples\")\n",
    "\n",
    "# Learning curve steepness\n",
    "print(\"\\\\nLearning Curve Analysis:\")\n",
    "print(\"-\" * 25)\n",
    "for method in comparison_results['method'].unique():\n",
    "    method_data = comparison_results[comparison_results['method'] == method]\n",
    "    initial_acc = method_data['accuracy'].iloc[0]\n",
    "    final_acc = method_data['accuracy'].iloc[-1]\n",
    "    improvement = final_acc - initial_acc\n",
    "    print(f\"{method.title():15s}: {improvement:.4f} improvement ({initial_acc:.4f} → {final_acc:.4f})\")\n",
    "\n",
    "# Statistical significance (simple comparison)\n",
    "print(\"\\\\nMethod Rankings by Final Performance:\")\n",
    "print(\"-\" * 35)\n",
    "final_perf = comparison_results.groupby('method')['accuracy'].last().sort_values(ascending=False)\n",
    "for i, (method, acc) in enumerate(final_perf.items()):\n",
    "    print(f\"{i+1}. {method.title():15s}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. QUALITATIVE ANALYSIS OF LLM SELECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_llm_selections(texts, labels, difficulty_levels):\n",
    "    \"\"\"\n",
    "    Analyze what types of samples the LLM tends to select\n",
    "    \"\"\"\n",
    "    if llm_model is None:\n",
    "        print(\"LLM model not available for qualitative analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"QUALITATIVE ANALYSIS OF LLM SELECTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    llm_selector = LLMQuerySelector(llm_tokenizer, llm_model, device)\n",
    "    \n",
    "    # Sample a subset for analysis\n",
    "    sample_indices = random.sample(range(len(texts)), 100)\n",
    "    sample_texts = [texts[i] for i in sample_indices]\n",
    "    sample_labels = [labels[i] for i in sample_indices]\n",
    "    sample_difficulties = [difficulty_levels[i] for i in sample_indices]\n",
    "    \n",
    "    strategies = ['informativeness', 'diversity', 'difficulty']\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"\\\\n--- {strategy.upper()} STRATEGY ---\")\n",
    "        \n",
    "        selected_indices = llm_selector.select_informative_samples(\n",
    "            sample_texts, n_select=10, strategy=strategy\n",
    "        )\n",
    "        \n",
    "        selected_texts = [sample_texts[i] for i in selected_indices]\n",
    "        selected_labels = [sample_labels[i] for i in selected_indices]\n",
    "        selected_difficulties = [sample_difficulties[i] for i in selected_indices]\n",
    "        \n",
    "        # Analyze selections\n",
    "        label_dist = pd.Series(selected_labels).value_counts()\n",
    "        difficulty_dist = pd.Series(selected_difficulties).value_counts()\n",
    "        \n",
    "        print(f\"Label distribution: {label_dist.to_dict()}\")\n",
    "        print(f\"Difficulty distribution: {difficulty_dist.to_dict()}\")\n",
    "        \n",
    "        print(\"\\\\nSelected samples:\")\n",
    "        for i, (text, label, diff) in enumerate(zip(selected_texts[:3], selected_labels[:3], selected_difficulties[:3])):\n",
    "            print(f\"{i+1}. [{label}|{diff}] {text[:80]}...\")\n",
    "\n",
    "# Run qualitative analysis\n",
    "analyze_llm_selections(texts, labels, difficulty)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
